{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make sure to also output the intermediary steps\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto-Cleaning Dirty Data: the Data Encoding Bot\n",
    "\n",
    "[2IMM00] Seminar Data Mining\n",
    "<br>\n",
    "Angelo Majoor - 1030843\n",
    "<br>\n",
    "A.R.Majoor@student.tue.nl\n",
    "\n",
    "Supervisor: dr. ir. J. (Joaquin) Vanschoren\n",
    "\n",
    "Eindhoven University of Technology\n",
    "<br>\n",
    "Department of Mathematics and Computer Science\n",
    "<br>\n",
    "Data Mining Research Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import all relevant libraries\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "import openml as oml\n",
    "oml.config.apikey = '2d4efc0fbf4c75a890be14297c5ec1e4'\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The three different steps\n",
    "\n",
    "As stated in the report, the process of creating the data encoding bot consists of three different steps:\n",
    "\n",
    " - Auto-detecting data types per feature (column)\n",
    " - Auto-detecting numeric, ordinal, categorical (integer) features\n",
    " - Auto-selecting encoding techniques for all features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Import the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>country</th>\n",
       "      <th>description</th>\n",
       "      <th>designation</th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>province</th>\n",
       "      <th>region_1</th>\n",
       "      <th>region_2</th>\n",
       "      <th>variety</th>\n",
       "      <th>winery</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>This tremendous 100% varietal wine hails from ...</td>\n",
       "      <td>Martha's Vineyard</td>\n",
       "      <td>96</td>\n",
       "      <td>235.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Napa Valley</td>\n",
       "      <td>Napa</td>\n",
       "      <td>Cabernet Sauvignon</td>\n",
       "      <td>Heitz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Ripe aromas of fig, blackberry and cassis are ...</td>\n",
       "      <td>Carodorum Selección Especial Reserva</td>\n",
       "      <td>96</td>\n",
       "      <td>110.0</td>\n",
       "      <td>Northern Spain</td>\n",
       "      <td>Toro</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tinta de Toro</td>\n",
       "      <td>Bodega Carmen Rodríguez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>US</td>\n",
       "      <td>Mac Watson honors the memory of a wine once ma...</td>\n",
       "      <td>Special Selected Late Harvest</td>\n",
       "      <td>96</td>\n",
       "      <td>90.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Knights Valley</td>\n",
       "      <td>Sonoma</td>\n",
       "      <td>Sauvignon Blanc</td>\n",
       "      <td>Macauley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>US</td>\n",
       "      <td>This spent 20 months in 30% new French oak, an...</td>\n",
       "      <td>Reserve</td>\n",
       "      <td>96</td>\n",
       "      <td>65.0</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>Ponzi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>France</td>\n",
       "      <td>This is the top wine from La Bégude, named aft...</td>\n",
       "      <td>La Brûlade</td>\n",
       "      <td>95</td>\n",
       "      <td>66.0</td>\n",
       "      <td>Provence</td>\n",
       "      <td>Bandol</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Provence red blend</td>\n",
       "      <td>Domaine de la Bégude</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 country                                        description  \\\n",
       "0           0      US  This tremendous 100% varietal wine hails from ...   \n",
       "1           1   Spain  Ripe aromas of fig, blackberry and cassis are ...   \n",
       "2           2      US  Mac Watson honors the memory of a wine once ma...   \n",
       "3           3      US  This spent 20 months in 30% new French oak, an...   \n",
       "4           4  France  This is the top wine from La Bégude, named aft...   \n",
       "\n",
       "                            designation  points  price        province  \\\n",
       "0                     Martha's Vineyard      96  235.0      California   \n",
       "1  Carodorum Selección Especial Reserva      96  110.0  Northern Spain   \n",
       "2         Special Selected Late Harvest      96   90.0      California   \n",
       "3                               Reserve      96   65.0          Oregon   \n",
       "4                            La Brûlade      95   66.0        Provence   \n",
       "\n",
       "            region_1           region_2             variety  \\\n",
       "0        Napa Valley               Napa  Cabernet Sauvignon   \n",
       "1               Toro                NaN       Tinta de Toro   \n",
       "2     Knights Valley             Sonoma     Sauvignon Blanc   \n",
       "3  Willamette Valley  Willamette Valley          Pinot Noir   \n",
       "4             Bandol                NaN  Provence red blend   \n",
       "\n",
       "                    winery  \n",
       "0                    Heitz  \n",
       "1  Bodega Carmen Rodríguez  \n",
       "2                 Macauley  \n",
       "3                    Ponzi  \n",
       "4     Domaine de la Bégude  "
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Unnamed: 0       int64\n",
       "country         object\n",
       "description     object\n",
       "designation     object\n",
       "points           int64\n",
       "price          float64\n",
       "province        object\n",
       "region_1        object\n",
       "region_2        object\n",
       "variety         object\n",
       "winery          object\n",
       "dtype: object"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#raw_data = pd.read_csv(\"ENTER_YOUR_CSV_FILE_NAME_HERE\")\n",
    "raw_data = pd.read_csv(\"Data/wine_mag.csv\")\n",
    "raw_data.head()\n",
    "raw_data.dtypes\n",
    "\n",
    "#raw_energy = pd.read_csv(\"Data/energy_train_copy.csv\")\n",
    "#raw_energy.head()\n",
    "#raw_energy.dtypes\n",
    "\n",
    "#raw_weather = pd.read_csv(\"Data/weather_train_copy.csv\")\n",
    "#raw_weather.head()\n",
    "#raw_weather.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Auto-Detecting Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to automatically infer data types for a specific feature that has the standard 'object' data type\n",
    "# Data types that we want to infer: boolean, date, float, integer, string\n",
    "# Note that every feature that is not either a boolean, a date, a float or an integer, is classified as a string\n",
    "# Input: Pandas Dataframe consisting of one single feature (so n*1 in size)\n",
    "# Output: Data type of the feature (in string format)\n",
    "\n",
    "def autoInferObject(raw_data_feature):\n",
    "    dataType = \"\"\n",
    "    types = [\"date\",\"float64\",\"int64\",\"string\"] #Data types\n",
    "    weights = [0,0,0,0] #Weights corresponding to the data types\n",
    "    numberOfIndices = 100 # umber of different values to check in a feature\n",
    "    \n",
    "    featureLength = len(raw_data_feature) #Number of rows in the feature\n",
    "    \n",
    "    randomIndices = random.sample(range(0,featureLength), min(numberOfIndices,featureLength)) #Array of random indices\n",
    "    \n",
    "    # If the feature only contains two different unique values, then infer it as boolean\n",
    "    if len(pd.unique(raw_data_feature)) == 2:\n",
    "        dataType = \"bool\"\n",
    "    else:\n",
    "        for i in randomIndices:\n",
    "            try:\n",
    "                if (len(raw_data_feature[i]) <= 10 \n",
    "                    and ((raw_data_feature[i][2:3] == ('-' or '/') and raw_data_feature[i][5:6] == ('-' or '/')) or \n",
    "                    (raw_data_feature[i][4:5] == ('-' or '/') and raw_data_feature[i][7:8] == ('-' or '/')))):\n",
    "                    weights[0] += 1 #Date\n",
    "                else:\n",
    "                    weights[3] += 1 #String\n",
    "            except (TypeError,ValueError,IndexError):\n",
    "                try:\n",
    "                    int(raw_data_feature[i])\n",
    "                    if ('.' in str(raw_data_feature[i])):\n",
    "                        weights[1] += 1 #Float\n",
    "                    else:\n",
    "                        weights[2] += 1 #Integer\n",
    "                except (TypeError,ValueError,IndexError):\n",
    "                    weights[3] += 1 #String\n",
    "    \n",
    "        #print (\"Date: {}, Float64: {}, Int64: {}, String: {}\".format(weights[0],weights[1],weights[2],weights[3])) #For debugging purposes\n",
    "        dataType = types[weights.index(max(weights))]\n",
    "        \n",
    "    return dataType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to automatically infer data types for every single feature in a raw data set\n",
    "# Input: Pandas Dataframe created directly from the raw data with the pd.read_csv function\n",
    "# Output: List of data types, one data type for each feature\n",
    "\n",
    "def autoDetectDataTypes(raw_data):\n",
    "    result = []\n",
    "    \n",
    "    for column in raw_data:\n",
    "        if raw_data.dtypes[column] == \"object\":\n",
    "            #print (\"Trying to automatically infer the data type of the\",column,\"feature...\") #For debugging purposes\n",
    "            inferredType = autoInferObject(raw_data[column])\n",
    "            result.append(inferredType)\n",
    "            #print (\"Result:\",inferredType) #For debugging purposes\n",
    "            # Auto-infer in step 1\n",
    "        elif raw_data.dtypes[column] == \"int64\":\n",
    "            result.append(\"int64\")\n",
    "            # Go to step 2\n",
    "        else:\n",
    "            # The only remaining data type is 'float64', which needs no special treatment\n",
    "            result.append(\"float64\")\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted data types:\n",
      " ['int64', 'string', 'string', 'string', 'int64', 'float64', 'string', 'string', 'string', 'string', 'string']\n"
     ]
    }
   ],
   "source": [
    "predicted = autoDetectDataTypes(raw_data)\n",
    "print (\"\\nPredicted data types:\\n\",predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the accuracy of the implemented solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['int64',\n",
       " 'string',\n",
       " 'string',\n",
       " 'string',\n",
       " 'int64',\n",
       " 'float64',\n",
       " 'string',\n",
       " 'string',\n",
       " 'string',\n",
       " 'string',\n",
       " 'string']"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manually check for the ground truth, since a dirty data set has no ground truth included\n",
    "\n",
    "#ground_truth_energy = ['date', 'string', 'int64', 'float64', 'float64', 'int64', 'float64']\n",
    "#ground_truth_weather = ['string', 'float64', 'float64', 'float64', 'float64', 'string', 'float64', 'float64', 'float64']\n",
    "#ground_truth_TED = ['int64', 'string', 'int64', 'string', 'int64', 'int64', 'string', 'string', 'int64', 'int64', 'string', 'string', 'string', 'string', 'string', 'string', 'int64']\n",
    "#ground_truth_wine = ['int64', 'string', 'string', 'string', 'int64', 'float64', 'string', 'string', 'string', 'string', 'string']\n",
    "\n",
    "ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to calculate an accuracy score for the implemented solution\n",
    "# Input: Array containing the (self-made) ground truth\n",
    "#        Array containing the predicted data types\n",
    "# Output: Accuracy score based on the number of correct predictions\n",
    "\n",
    "def score(ground_truth, predicted):\n",
    "    correctPredictions = 0\n",
    "    \n",
    "    for i in range(0,len(ground_truth)):\n",
    "        if ground_truth[i] == predicted[i]:\n",
    "            correctPredictions += 1\n",
    "        \n",
    "    return correctPredictions / len(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "print (\"Accuracy:\",score(ground_truth, predicted)*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data Set</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Retrieved From</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Solar Panel Energy Production Eindhoven</td>\n",
       "      <td>100.0</td>\n",
       "      <td>https://canvas.tue.nl/files/508283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Weather Measurements Eindhoven Airport</td>\n",
       "      <td>100.0</td>\n",
       "      <td>https://canvas.tue.nl/files/508283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TED Talks</td>\n",
       "      <td>100.0</td>\n",
       "      <td>https://www.kaggle.com/rounakbanik/ted-talks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wine Reviews</td>\n",
       "      <td>100.0</td>\n",
       "      <td>https://www.kaggle.com/zynicide/wine-reviews</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Data Set  Accuracy  \\\n",
       "0  Solar Panel Energy Production Eindhoven     100.0   \n",
       "1   Weather Measurements Eindhoven Airport     100.0   \n",
       "2                                TED Talks     100.0   \n",
       "3                             Wine Reviews     100.0   \n",
       "\n",
       "                                 Retrieved From  \n",
       "0            https://canvas.tue.nl/files/508283  \n",
       "1            https://canvas.tue.nl/files/508283  \n",
       "2  https://www.kaggle.com/rounakbanik/ted-talks  \n",
       "3  https://www.kaggle.com/zynicide/wine-reviews  "
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scoreResults = [(\"Solar Panel Energy Production Eindhoven\",100.0,\"https://canvas.tue.nl/files/508283\"), \\\n",
    "                (\"Weather Measurements Eindhoven Airport\",100.0,\"https://canvas.tue.nl/files/508283\"), \\\n",
    "                (\"TED Talks\",100.0,\"https://www.kaggle.com/rounakbanik/ted-talks\"), \\\n",
    "                (\"Wine Reviews\",100.0,\"https://www.kaggle.com/zynicide/wine-reviews\")]\n",
    "pd.DataFrame(scoreResults, columns=[\"Data Set\", \"Accuracy\", \"Retrieved From\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flaws\n",
    "\n",
    " - Whenever a feature is recognized as integer by the read_csv() function, this feature cannot be a boolean anymore. This can simply be solved by adding a check for the number of unique elements in the 'autoDetectDataTypes()' function, in the 'elif' branch (where the function checks for int64's).\n",
    " - Dates that are represented as integers cannot be detected, since it is unknown whether a value represents a date, or an actual number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 Completed --------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Numeric, Categorical or Ordinal Data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kr-vs-kp\n",
      "letter\n",
      "balance-scale\n",
      "mfeat-factors\n",
      "mfeat-fourier\n",
      "breast-w\n",
      "mfeat-karhunen\n",
      "mfeat-morphological\n",
      "mfeat-pixel\n",
      "car\n",
      "mfeat-zernike\n",
      "cmc\n",
      "mushroom\n",
      "optdigits\n",
      "credit-approval\n",
      "credit-g\n",
      "pendigits\n",
      "segment\n",
      "diabetes\n",
      "soybean\n",
      "spambase\n",
      "splice\n",
      "tic-tac-toe\n",
      "vehicle\n",
      "waveform-5000\n",
      "electricity\n",
      "satimage\n",
      "eucalyptus\n",
      "sick\n",
      "vowel\n",
      "isolet\n",
      "scene\n",
      "monks-problems-1\n",
      "monks-problems-2\n",
      "monks-problems-3\n",
      "JapaneseVowels\n",
      "synthetic_control\n",
      "irish\n",
      "analcatdata_authorship\n",
      "analcatdata_dmft\n",
      "profb\n",
      "collins\n",
      "mnist_784\n",
      "sylva_agnostic\n",
      "gina_agnostic\n",
      "ada_agnostic\n",
      "mozilla4\n",
      "pc4\n",
      "pc3\n",
      "jm1\n",
      "kc2\n",
      "kc1\n",
      "pc1\n",
      "KDDCup09_churn\n",
      "KDDCup09_upselling\n",
      "MagicTelescope\n",
      "adult\n",
      "wilt\n",
      "wdbc\n",
      "micro-mass\n",
      "phoneme\n",
      "one-hundred-plants-margin\n",
      "one-hundred-plants-shape\n",
      "one-hundred-plants-texture\n",
      "qsar-biodeg\n",
      "wall-robot-navigation\n",
      "semeion\n",
      "steel-plates-fault\n",
      "tamilnadu-electricity\n",
      "hill-valley\n",
      "ilpd\n",
      "madelon\n",
      "nomao\n",
      "ozone-level-8hr\n",
      "cardiotocography\n",
      "climate-model-simulation-crashes\n",
      "cnae-9\n",
      "eeg-eye-state\n",
      "first-order-theorem-proving\n",
      "gas-drift\n",
      "banknote-authentication\n",
      "blood-transfusion-service-center\n",
      "artificial-characters\n",
      "bank-marketing\n",
      "Bioresponse\n",
      "cjs\n",
      "cylinder-bands\n",
      "GesturePhaseSegmentationProcessed\n",
      "har\n",
      "Internet-Advertisements\n",
      "PhishingWebsites\n",
      "MiceProtein\n",
      "Amazon_employee_access\n",
      "dresses-sales\n",
      "LED-display-domain-7digit\n",
      "texture\n",
      "Australian\n",
      "connect-4\n",
      "higgs\n",
      "SpeedDating\n"
     ]
    }
   ],
   "source": [
    "# Load the OpenML100 data set\n",
    "benchmark_suite = oml.study.get_study('OpenML100','tasks')\n",
    "\n",
    "list_task_ids = []\n",
    "\n",
    "# Find all task_ids\n",
    "for task_id in benchmark_suite.tasks:\n",
    "    list_task_ids.append(task_id)\n",
    "    task = oml.tasks.get_task(task_id)\n",
    "    print (task.get_dataset().name)\n",
    "\n",
    "#list_task_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mfeat-factors\n",
      "[   98.   236.   531.   673.   607.   647.     2.     9.     3.     6.\n",
      "     8.     5.   225.   517.   652.   624.   628.   994.     7.    22.\n",
      "    28.    13.    10.    19.   305.   481.   667.   663.  1009.   727.\n",
      "    38.    28.    18.    11.    20.    10.   287.   567.   651.   742.\n",
      "   824.   900.    26.    34.    30.     8.    16.    13.   248.   556.\n",
      "   631.   796.   926.   748.    39.    34.    18.     9.    17.    12.\n",
      "   248.   540.   506.   814.  1051.   728.    38.    28.     5.    13.\n",
      "    16.     8.   246.   518.   751.   579.   699.  1062.    13.    30.\n",
      "    28.    10.    16.    16.   276.   344.   682.   500.   709.   916.\n",
      "    10.    30.    23.    17.    15.    14.   357.   435.   829.   610.\n",
      "   745.   994.    20.     7.    24.    12.    10.     9.   355.   409.\n",
      "   477.   886.   976.   723.    30.    24.    14.     7.     7.     8.\n",
      "   290.   352.   435.   753.   894.   751.    29.    29.     2.    13.\n",
      "    13.    14.   260.   286.   562.   698.   665.   757.    11.     8.\n",
      "    15.    14.     9.     9.   238.   292.   586.   698.   733.   707.\n",
      "     9.     6.    15.    11.    10.    16.   294.   406.   654.   644.\n",
      "   741.  1000.    18.    23.    17.     9.    20.    11.   302.   418.\n",
      "   561.   709.   961.   776.    21.    25.    12.    12.    19.    10.\n",
      "   360.   328.   607.   984.  1186.   599.    29.     7.    14.     6.\n",
      "     9.     9.   362.   314.   924.   733.   601.  1216.     4.     8.\n",
      "    20.    10.     9.     5.   251.   421.   474.   536.   628.   632.\n",
      "    18.    36.     8.    15.    12.    13.]\n",
      "0\n",
      "[  121.   193.   607.   611.   585.   665.     7.     9.     2.     4.\n",
      "     3.     7.   214.   514.   690.   548.   630.  1006.     2.    18.\n",
      "    31.    15.     5.    15.   260.   344.   655.   605.   993.   743.\n",
      "    29.    32.    25.    11.    17.     6.   308.   412.   719.   700.\n",
      "   784.   902.    31.    32.    37.     8.    13.    15.   259.   429.\n",
      "   699.   758.   900.   762.    34.    30.    25.     9.    14.    10.\n",
      "   207.   445.   558.   784.  1037.   750.    29.    30.    12.    11.\n",
      "    17.     6.   199.   427.   791.   553.   707.  1074.    10.    26.\n",
      "    31.    12.    17.    10.   253.   345.   726.   440.   731.   932.\n",
      "     7.    26.    30.    19.    10.    12.   326.   452.   795.   540.\n",
      "   701.  1012.    17.     7.    31.    12.    11.    11.   320.   264.\n",
      "   563.   796.   972.   741.    29.    20.    17.     7.     8.     6.\n",
      "   243.   255.   467.   711.   896.   769.    26.    25.     9.     9.\n",
      "    12.    10.   265.   171.   648.   626.   687.   777.    16.    10.\n",
      "    12.    10.     4.     5.   279.   269.   634.   628.   737.   727.\n",
      "    14.    10.    12.    13.     5.    14.   253.   317.   680.   614.\n",
      "   755.  1016.    15.    19.    24.    11.    17.    11.   255.   321.\n",
      "   617.   651.   979.   792.    20.    29.    19.     8.    18.     8.\n",
      "   351.   249.   687.   922.  1194.   617.    26.    11.    11.     4.\n",
      "    10.     7.   347.   439.   950.   675.   577.  1222.    13.     6.\n",
      "    27.    10.    10.     3.   224.   354.   520.   458.   570.   634.\n",
      "    15.    32.    11.    13.    15.    11.]\n",
      "0\n",
      "[  115.   141.   590.   605.   557.   627.    12.     6.     3.     3.\n",
      "     5.     4.   196.   404.   611.   560.   594.   986.     7.    21.\n",
      "    28.    14.     7.    12.   278.   406.   670.   579.   991.   703.\n",
      "    24.    29.    18.    12.    19.     9.   378.   468.   668.   654.\n",
      "   786.   900.    32.    31.    28.    11.    17.    12.   299.   495.\n",
      "   650.   698.   902.   726.    29.    31.    18.    12.    16.     5.\n",
      "   249.   389.   465.   722.  1033.   704.    24.    27.     5.    12.\n",
      "    17.    11.   229.   377.   700.   483.   675.  1056.     5.    29.\n",
      "    28.    11.    17.    15.   247.   309.   645.   444.   697.   898.\n",
      "     6.    29.    21.    18.    12.     7.   312.   328.   774.   550.\n",
      "   689.   980.    14.     6.    20.    11.     9.     6.   360.   294.\n",
      "   568.   790.   956.   733.    24.    23.    18.     6.     8.    11.\n",
      "   235.   259.   436.   665.   876.   719.    21.    28.     0.    10.\n",
      "    14.    13.   253.   111.   653.   638.   653.   737.    17.     9.\n",
      "    15.    11.     6.    10.   297.   185.   637.   638.   715.   677.\n",
      "     9.     9.    15.    12.     7.     9.   271.   335.   591.   542.\n",
      "   719.   970.    10.    22.    13.    12.    19.     6.   293.   335.\n",
      "   536.   613.   943.   746.    15.    26.    10.     9.    20.    13.\n",
      "   415.   323.   672.   896.  1168.   595.    21.     8.    14.     3.\n",
      "    10.    12.   343.   365.   877.   663.   547.  1192.    18.     7.\n",
      "    20.    13.    10.     2.   196.   348.   535.   498.   572.   656.\n",
      "    20.    35.    16.    14.    13.     6.]\n",
      "0\n",
      "[   90.   122.   627.   692.   607.   642.     0.     6.     4.     5.\n",
      "     3.     5.   201.   445.   664.   629.   626.   945.     5.    21.\n",
      "    27.    16.     7.     9.   273.   397.   703.   662.   987.   710.\n",
      "    36.    29.    21.     8.    15.    10.   331.   459.   731.   765.\n",
      "   818.   865.    24.    29.    33.     9.    15.    11.   256.   510.\n",
      "   707.   801.   900.   735.    37.    29.    21.    10.    16.     2.\n",
      "   246.   438.   534.   817.  1031.   721.    36.    27.     8.    10.\n",
      "    15.     8.   248.   390.   767.   586.   693.  1013.    11.    29.\n",
      "    27.    13.    15.    12.   278.   312.   702.   509.   707.   873.\n",
      "     8.    29.    26.    18.     8.     8.   361.   347.   831.   631.\n",
      "   739.   955.    18.     8.    27.    13.    13.     9.   369.   293.\n",
      "   589.   879.   962.   718.    28.    23.    13.     8.    10.     8.\n",
      "   268.   364.   449.   754.   884.   728.    27.    28.     5.     8.\n",
      "    10.    14.   232.   172.   674.   713.   663.   722.     9.     9.\n",
      "    14.     9.     2.     9.   244.   202.   682.   719.   725.   684.\n",
      "     7.     9.    16.    14.     5.     6.   260.   412.   640.   651.\n",
      "   735.   973.    16.    22.    20.    12.    15.     3.   276.   416.\n",
      "   601.   712.   959.   757.    19.    26.    15.     7.    16.    10.\n",
      "   380.   354.   711.   987.  1180.   606.    27.     8.    15.     5.\n",
      "    12.     9.   400.   358.   918.   756.   603.  1171.     6.     7.\n",
      "    23.    11.    12.     5.   263.   419.   576.   549.   628.   621.\n",
      "    16.    35.     7.    12.    15.     9.]\n",
      "0\n",
      "[  1.57000000e+02   1.67000000e+02   6.81000000e+02   6.66000000e+02\n",
      "   5.87000000e+02   6.66000000e+02   8.00000000e+00   6.00000000e+00\n",
      "   1.00000000e+00   4.00000000e+00   5.00000000e+00   5.00000000e+00\n",
      "   2.12000000e+02   3.86000000e+02   7.40000000e+02   6.13000000e+02\n",
      "   6.14000000e+02   9.97000000e+02   5.00000000e+00   2.10000000e+01\n",
      "   3.00000000e+01   1.10000000e+01   7.00000000e+00   1.70000000e+01\n",
      "   2.48000000e+02   3.90000000e+02   7.03000000e+02   6.62000000e+02\n",
      "   9.49000000e+02   7.44000000e+02   2.80000000e+01   2.90000000e+01\n",
      "   2.00000000e+01   1.30000000e+01   1.90000000e+01   1.00000000e+01\n",
      "   2.44000000e+02   4.68000000e+02   7.97000000e+02   7.43000000e+02\n",
      "   7.50000000e+02   9.09000000e+02   3.20000000e+01   2.90000000e+01\n",
      "   3.20000000e+01   1.00000000e+01   1.50000000e+01   1.10000000e+01\n",
      "   2.31000000e+02   4.79000000e+02   7.73000000e+02   7.99000000e+02\n",
      "   8.50000000e+02   7.69000000e+02   3.70000000e+01   2.90000000e+01\n",
      "   2.00000000e+01   1.10000000e+01   1.40000000e+01   1.00000000e+01\n",
      "   1.89000000e+02   3.61000000e+02   6.20000000e+02   8.19000000e+02\n",
      "   9.95000000e+02   7.43000000e+02   3.20000000e+01   2.70000000e+01\n",
      "   7.00000000e+00   1.50000000e+01   1.70000000e+01   8.00000000e+00\n",
      "   1.83000000e+02   3.65000000e+02   8.51000000e+02   5.88000000e+02\n",
      "   7.01000000e+02   1.06300000e+03   1.30000000e+01   2.90000000e+01\n",
      "   3.00000000e+01   8.00000000e+00   1.70000000e+01   1.40000000e+01\n",
      "   2.57000000e+02   2.37000000e+02   7.82000000e+02   5.01000000e+02\n",
      "   7.15000000e+02   9.25000000e+02   1.00000000e+01   3.10000000e+01\n",
      "   2.50000000e+01   1.50000000e+01   1.20000000e+01   1.20000000e+01\n",
      "   3.14000000e+02   3.34000000e+02   8.47000000e+02   5.99000000e+02\n",
      "   6.89000000e+02   1.00300000e+03   1.60000000e+01   8.00000000e+00\n",
      "   2.60000000e+01   8.00000000e+00   9.00000000e+00   1.30000000e+01\n",
      "   3.10000000e+02   3.00000000e+02   6.33000000e+02   8.71000000e+02\n",
      "   9.36000000e+02   7.52000000e+02   3.20000000e+01   2.30000000e+01\n",
      "   1.60000000e+01   7.00000000e+00   8.00000000e+00   8.00000000e+00\n",
      "   2.57000000e+02   2.33000000e+02   5.11000000e+02   7.56000000e+02\n",
      "   8.60000000e+02   7.58000000e+02   2.90000000e+01   3.00000000e+01\n",
      "   4.00000000e+00   1.10000000e+01   1.40000000e+01   1.40000000e+01\n",
      "   2.65000000e+02   1.37000000e+02   7.18000000e+02   6.87000000e+02\n",
      "   6.71000000e+02   7.68000000e+02   1.70000000e+01   9.00000000e+00\n",
      "   1.30000000e+01   1.00000000e+01   6.00000000e+00   9.00000000e+00\n",
      "   2.53000000e+02   1.69000000e+02   7.04000000e+02   6.87000000e+02\n",
      "   7.07000000e+02   7.14000000e+02   1.50000000e+01   9.00000000e+00\n",
      "   1.30000000e+01   9.00000000e+00   7.00000000e+00   1.40000000e+01\n",
      "   2.51000000e+02   2.83000000e+02   7.14000000e+02   6.51000000e+02\n",
      "   7.33000000e+02   1.00900000e+03   1.80000000e+01   2.20000000e+01\n",
      "   1.90000000e+01   7.00000000e+00   1.90000000e+01   9.00000000e+00\n",
      "   2.39000000e+02   2.81000000e+02   6.85000000e+02   7.16000000e+02\n",
      "   9.55000000e+02   7.85000000e+02   2.30000000e+01   2.60000000e+01\n",
      "   1.40000000e+01   8.00000000e+00   2.00000000e+01   6.00000000e+00\n",
      "   3.37000000e+02   3.03000000e+02   7.63000000e+02   9.89000000e+02\n",
      "   1.15600000e+03   6.28000000e+02   2.90000000e+01   8.00000000e+00\n",
      "   1.20000000e+01   2.00000000e+00   1.00000000e+01   7.00000000e+00\n",
      "   3.21000000e+02   3.37000000e+02   9.88000000e+02   7.26000000e+02\n",
      "   5.77000000e+02   1.22500000e+03   1.40000000e+01   7.00000000e+00\n",
      "   2.20000000e+01   8.00000000e+00   1.00000000e+01   5.00000000e+00\n",
      "   2.76000000e+02   3.42000000e+02   5.94000000e+02   5.25000000e+02\n",
      "   5.68000000e+02   6.53000000e+02   1.60000000e+01   3.50000000e+01\n",
      "   1.00000000e+01   1.50000000e+01   1.30000000e+01   1.30000000e+01]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "trial = oml.tasks.get_task(list_task_ids[3])\n",
    "print (trial.get_dataset().name)\n",
    "\n",
    "X, y = trial.get_X_and_y()\n",
    "for i in range (0,5):\n",
    "    print (X[i])\n",
    "    print (y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to check if a feature contains categorical data, specifically concerning strings\n",
    "# If a feature contains at most 25 unique elements, this feature is always considered as categorical\n",
    "# If a feature contains between 26 and k (where k is user specified) unique values, then the function \n",
    "# calculates a similarity score between all of the different values. If this score is higher than 0.70 \n",
    "# a feature is also considered to be categorical (since the values at least have some relationship).\n",
    "# Input: Pandas Dataframe consisting of one single feature (so n*1 in size)\n",
    "#        A user-determined value k (the critical value: more than k unique values cannot be categorical)\n",
    "# Output: A boolean stating whether the supplied feature is categorical or not\n",
    "\n",
    "def autoCheckCategoricalString(raw_data_feature, k):\n",
    "    categorical = False\n",
    "    \n",
    "    allWords = pd.unique(raw_data_feature) #All unique strings in the feature\n",
    "    similarityScores = []\n",
    "    \n",
    "    if (len(allWords) <= 25): #If there are less than 25 unique strings, it is categorical\n",
    "        categorical = True\n",
    "        \n",
    "    elif (len(allWords) <= k): #Else if there are less than k unique strings, execute the following code\n",
    "        \n",
    "        for i in range(0,len(allWords)-2):\n",
    "            for j in range(i+1,len(allWords)-1):\n",
    "                if (pd.isnull(allWords[i])): #If a string has no value (NaN), turn it into some nonsense\n",
    "                    allWords[i] = \"abcdef\"\n",
    "                elif (pd.isnull(allWords[j])):\n",
    "                    allWords[j] = \"abcdef\"\n",
    "            \n",
    "                word_1 = wn.synsets(allWords[i])\n",
    "                word_2 = wn.synsets(allWords[j])\n",
    "            \n",
    "                if (word_1 != [] and word_2 != []): #Calculate similarity between two non-empty words\n",
    "                    similarity = wn.wup_similarity(word_1[0], word_2[0])\n",
    "                    #print (\"Similarity between\",word_1[0],\"and\",word_2[0],\":\",similarity) #For debugging purposes\n",
    "                    similarityScores.append(similarity)\n",
    "        \n",
    "        # Use this part of the code if you do not want to calculate a similarity score between\n",
    "        # every single string, but only every string with one single other string.\n",
    "        #\n",
    "        #for i in range(0,len(allWords)-1):\n",
    "        #    if (pd.isnull(allWords[i])):\n",
    "        #        allWords[i] = \"abcdef\"\n",
    "        #    elif (pd.isnull(allWords[i+1])):\n",
    "        #        allWords[i+1] = \"abcdef\"\n",
    "        #    \n",
    "        #    word_1 = wn.synsets(allWords[i])\n",
    "        #    word_2 = wn.synsets(allWords[i+1])\n",
    "        #    \n",
    "        #    if (word_1 != [] and word_2 != []):\n",
    "        #        similarity = wn.wup_similarity(word_1[0], word_2[0])\n",
    "        #        print (\"Similarity between\",word_1[0],\"and\",word_2[0],\":\",similarity)\n",
    "        #        similarityScores.append(similarity)\n",
    "    \n",
    "        #print (\"Similarity Scores:\\n\",similarityScores) #For debugging purposes\n",
    "        print (\"Mean Similarity Score:\",np.mean(similarityScores))\n",
    "    \n",
    "        if (np.mean(similarityScores) > 0.70): #0.70 = Critical similarity value\n",
    "            categorical = True\n",
    "    \n",
    "    print (\"Categorical Feature?\",categorical)\n",
    "    \n",
    "    return categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Similarity Score: 0.737473869897\n",
      "Categorical Feature? True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoCheckCategoricalString(raw_data[\"country\"],50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def autoCheckCategoricalInt(raw_data_feature, k):\n",
    "    categorical = False\n",
    "    \n",
    "    if (len(pd.unique(raw_data_feature)) <= k):\n",
    "        categorical = True\n",
    "\n",
    "    return categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def autoCheckCategoricals(raw_data,predicted,k):\n",
    "    \n",
    "    print (\"Checking if any of the features has categorical data...\")\n",
    "    \n",
    "    for j in range(0,len(predicted)):\n",
    "        if (predicted[j] == 'int64'):\n",
    "            if (autoCheckCategoricalInt(raw_data.iloc[:,j],k)):\n",
    "                predicted[j] = 'cat_int64'\n",
    "        elif (predicted[j] == 'string'):\n",
    "            if (autoCheckCategoricalString(raw_data.iloc[:,j],k)):\n",
    "                predicted[j] = 'cat_string'\n",
    "                \n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if any of the features has categorical data...\n",
      "Mean Similarity Score: 0.737473869897\n",
      "Categorical Feature? True\n",
      "Categorical Feature? False\n",
      "Categorical Feature? False\n",
      "Categorical Feature? False\n",
      "Categorical Feature? False\n",
      "Categorical Feature? True\n",
      "Categorical Feature? False\n",
      "Categorical Feature? False\n",
      "\n",
      "Predicted data types:\n",
      " ['int64', 'cat_string', 'string', 'string', 'cat_int64', 'float64', 'string', 'string', 'cat_string', 'string', 'string']\n"
     ]
    }
   ],
   "source": [
    "predicted = autoCheckCategoricals(raw_data,predicted,50)\n",
    "print (\"\\nPredicted data types:\\n\",predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('united_states.n.01'),\n",
       " Synset('uracil.n.01'),\n",
       " Synset('uranium.n.01'),\n",
       " Synset('u.n.03')]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[Synset('spain.n.01')]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[Synset('france.n.01'), Synset('france.n.02')]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[Synset('california.n.01')]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[Synset('oregon.n.01')]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[Synset('turkey.n.01'),\n",
       " Synset('turkey.n.02'),\n",
       " Synset('joker.n.02'),\n",
       " Synset('turkey.n.04'),\n",
       " Synset('turkey.n.05')]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.1111111111111111"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('wordnet')\n",
    "\n",
    "country_1 = wn.synsets(raw_data[\"country\"][0])\n",
    "country_2 = wn.synsets(raw_data[\"country\"][1])\n",
    "country_3 = wn.synsets(raw_data[\"country\"][4])\n",
    "country_1\n",
    "country_2\n",
    "country_3\n",
    "province_1 = wn.synsets(raw_data[\"province\"][0])\n",
    "province_1\n",
    "province_3 = wn.synsets(raw_data[\"province\"][3])\n",
    "province_3\n",
    "\n",
    "#wn.synset('united_states.n.01').path_similarity(wn.synset('spain.n.01'))\n",
    "wn.path_similarity(province_3[0],province_1[0])\n",
    "wn.wup_similarity(province_3[0],province_1[0])\n",
    "\n",
    "wn.synsets('turkey')\n",
    "\n",
    "wn.wup_similarity(wn.synset('india.n.01'),wn.synset('turkey.n.05'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
